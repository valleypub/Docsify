[toc]

## mrtk-webrtc github上的issue
1. how  to one-to-many connection by webrtc
- WebRTC is a peer-to-peer technology so there's no support for multiple peers at the protocol level. You should create one peer connection per pair of actors (PC-HoloLens).
- This is not supported as is by any WebRTC implementation, because this is not supported by the WebRTC standard. WebRTC is a peer-to-peer technology.
- Now, you can combine WebRTC -- and MixedReality-WebRTC will support that (in fact most implementations should) -- with a media server, some server-side software that will connect via peer-to-peer to all clients and forward to each and all the audio and video, with various rules. An example is Janus which I know for a fact works with MixedReality-WebRTC (disclaimer: we are not affiliated in any way with Janus, but this is the only one I know a bit to talk about). But you need to implement the signaling protocol of Janus and a way to manage the connection to your server. And of course have a server in the first place. What you are looking for in particular is their "video room" plugin (demo here and code here). Janus uses some JSON-based signaling, and you need in particular to handle the following JSON messages (non-exclusive list):

2. 如何用外置usb cam取代内置的webcam？
- At the moment there is no built-in support for Azure Kinect DK. You will need to manage the camera yourself and feed its content to an external video track source. But bear in mind that the video codecs usually are designed for RGB content, so might compress the depth data in a way that is not optimal and/or adds artifacts (see e.g. this article for an analysis about VP8 and alpha channel).
3. 如何 将 外置相机capture的image 添加进external source track
4. 
[](http://wiki.webmproject.org/alpha-channel)

## webrtc协议



## stun turn是什么



## 为什么unity下渲染vedio需要 mesh filter -> mesh render  
